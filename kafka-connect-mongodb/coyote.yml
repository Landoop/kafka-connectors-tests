- name: coyote
  title: kafka-connect-mongodb

- name: Setup Containers
  entries:
    - name: Docker Compose Pull
      command: docker-compose pull
      ignore_exit_code: true
    - name: Docker Compose Build
      command: docker-compose build
    - name: Docker Compose Up
      command: docker-compose up -d
    - name: Sleep a bit
      command: sleep 45
      nolog: true
    - name: Check docker compose log
      command: docker-compose logs
      stdout_has: [ 'INFO success: broker entered RUNNING state' ]
      stdout_not_has: [ 'INFO exited: broker', 'mongo.*ERROR:' ]

- name: Setup Mongo DB Connector
  entries:
    - name: Create Topic
      command: >
        docker run --rm --network=kafkaconnectmongodb_default landoop/fast-data-dev
          kafka-topics --zookeeper fast-data-dev:2181 --topic mongo-sink --partition 1 --replication 1 --create
    # - name: Setup Mongo DB
    #   command: docker exec -i kafkaconnectmongodb_mongo_1 mongo admin
    #   stdin: |
    #     db.createUser({ user: 'landoop', pwd: 'landoop', roles: [ { role: "userAdminAnyDatabase", db: "admin" } ] });
    - name: Create a Mongo DB Distributed Connector
      command: >
        docker run --rm --network=kafkaconnectmongodb_default -i landoop/fast-data-dev
          curl -vs --stderr - -X POST -H "Content-Type: application/json"
               --data @-
               "http://fast-data-dev:8083/connectors"
      stdout_not_has: [ 'HTTP/1.1 [45][0-9][0-9] ' ]
      stdin: |
        {
          "name": "mongo-sink",
          "config": {
            "connector.class": "com.datamountaineer.streamreactor.connect.mongodb.sink.MongoSinkConnector",
            "tasks.max": "1",
            "topics": "mongo-sink",
            "connect.mongo.database": "landoop",
            "connect.mongo.connection": "mongodb://mongo:27017/landoop",
            "connect.mongo.sink.batch.size": "10",
            "connect.mongo.sink.kcql": "INSERT INTO orders SELECT * FROM mongo-sink"
          }
        }
    - command: sleep 30
      nolog: true

- name: Test Connector
  entries:
    - name: Write Entries into Topic
      command: >
        docker run --rm -i --network=kafkaconnectmongodb_default landoop/fast-data-dev
          kafka-avro-console-producer --broker-list fast-data-dev:9092
            --topic mongo-sink --property schema.registry.url="http://fast-data-dev:8081"
            --property
            value.schema='{"type":"record","name":"myrecord","fields":[{"name":"id","type":"int"},{"name":"created", "type": "string"}, {"name":"product", "type": "string"}, {"name":"price", "type": "double"}]}'
      stdin: |
        {"id": 1, "created": "2016-05-06 13:53:00", "product": "OP-DAX-P-20150201-95.7", "price": 94.2}
        {"id": 2, "created": "2016-05-06 13:54:00", "product": "OP-DAX-C-20150201-100", "price": 99.5}
        {"id": 3, "created": "2016-05-06 13:55:00", "product": "FU-DATAMOUNTAINEER-20150201-100", "price": 10000}
        {"id": 4, "created": "2016-05-06 13:56:00", "product": "FU-KOSPI-C-20150201-100", "price": 150}
      timeout: 20s
    - command: sleep 60
      nolog: true
    - name: Verify Mongo DB
      command: docker exec -i kafkaconnectmongodb_mongo_1 mongo
      stdin: |
        use landoop
        db.orders.find()
      stdout_has: [ 'OP-DAX-P-20150201-95.7', 'FU-KOSPI-C-20150201-100', '2016-05-06 13:54:00', '2016-05-06 13:55:00' ]
    - name: Read First 3000 Lines of Connect Logs
      command: >
        docker exec kafkaconnectmongodb_fast-data-dev_1 head -n3000 /var/log/connect-distributed.log
      stdout_not_has: [ '\] ERROR' ]

- name: Clean-up Containers
  entries:
    - name: Docker Compose Down
      command: docker-compose down
